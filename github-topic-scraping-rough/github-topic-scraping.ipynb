{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top repositories of github topics\n",
    "\n",
    "\n",
    "# github-topic-scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"kumaranshu8092/github-topic-scraping\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/kumaranshu8092/github-topic-scraping\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/kumaranshu8092/github-topic-scraping'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this to save new versions of the notebook\n",
    "jovian.commit(project=\"github-topic-scraping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a website and describe your objective\n",
    "\n",
    "    -Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "    -Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "    -Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project path to follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics\n",
    "- We'll get a list of topics. for each topic, we'll get topic title,topic page URL and topic description\n",
    "- For each topic, we'll get the 25 repositories in the topic from the topic page\n",
    "- for each repository,we'll grab the repo name, username,starts and repo url\n",
    "- For each topic we'll create a CSV file ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the requests library to download web pages\n",
    "\n",
    "    Inspect the website's HTML source and identify the right URLs to download.\n",
    "    Download and save web pages locally using the requests library.\n",
    "    Create a function to automate downloading for different topics/search queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/topics' \n",
    "#url of the web page which we will be scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "#by using (.text) we get the page in text formate \n",
    "page_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141965"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to open the page in html formate\n",
    "with open(\"webpage.html\",'w') as f:\n",
    "    f.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Beautiful Soup to parse and extract information\n",
    "\n",
    "    Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "    Use the right properties and methods to extract the required information.\n",
    "    Create functions to extract from the page into lists and dictionaries.\n",
    "    (Optional) Use a REST API to acquire additional information if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_doc = BeautifulSoup(page_content,\"html.parser\")\n",
    "# now parsed_doc contains the whole page content in html parsed formate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = parsed_doc.find_all('p',{'class': 'f3 lh-condensed mb-0 mt-1 Link--primary'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_description = parsed_doc.find_all('p',{'class': 'f5 color-fg-muted mb-0 mt-1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_link = parsed_doc.find_all('a',{'class':'no-underline flex-1 d-flex flex-column'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topic_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the whole link to the 1st topic of topic page\n",
    "topic_link_0 = \"https://github.com\" + topic_link[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3D'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(topic_link_0)\n",
    "\n",
    "topic_name[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the details in different lists\n",
    "#topic name list\n",
    "topic_names=[]\n",
    "for n in topic_name:\n",
    "    topic_names.append(n.text)\n",
    "#topic description list    \n",
    "topic_descp=[]\n",
    "for d in topic_description:\n",
    "    topic_descp.append(d.text.strip())\n",
    "    \n",
    "topic_links = []\n",
    "for l in topic_link:\n",
    "    topic_links.append(\"https://github.com\" + l[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://github.com/topics/3d',\n",
       " 'https://github.com/topics/ajax',\n",
       " 'https://github.com/topics/algorithm',\n",
       " 'https://github.com/topics/amphp',\n",
       " 'https://github.com/topics/android',\n",
       " 'https://github.com/topics/angular',\n",
       " 'https://github.com/topics/ansible',\n",
       " 'https://github.com/topics/api',\n",
       " 'https://github.com/topics/arduino',\n",
       " 'https://github.com/topics/aspnet',\n",
       " 'https://github.com/topics/atom',\n",
       " 'https://github.com/topics/awesome',\n",
       " 'https://github.com/topics/aws',\n",
       " 'https://github.com/topics/azure',\n",
       " 'https://github.com/topics/babel',\n",
       " 'https://github.com/topics/bash',\n",
       " 'https://github.com/topics/bitcoin',\n",
       " 'https://github.com/topics/bootstrap',\n",
       " 'https://github.com/topics/bot',\n",
       " 'https://github.com/topics/c',\n",
       " 'https://github.com/topics/chrome',\n",
       " 'https://github.com/topics/chrome-extension',\n",
       " 'https://github.com/topics/cli',\n",
       " 'https://github.com/topics/clojure',\n",
       " 'https://github.com/topics/code-quality',\n",
       " 'https://github.com/topics/code-review',\n",
       " 'https://github.com/topics/compiler',\n",
       " 'https://github.com/topics/continuous-integration',\n",
       " 'https://github.com/topics/covid-19',\n",
       " 'https://github.com/topics/cpp']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Till now we have gotten the data of topic page\n",
    "## Now we will get the data of pages in topic pages\n",
    "\n",
    "    - we'll get the username, repo_name ,stars and repo_url from the inside topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_page_link = topic_links[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(repo_page_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before getting the class and details we have to parse the text formate page detail\n",
    "repo_parsed = BeautifulSoup(repo_page,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#username and repo name doesn't have any class so we have to get their parent class and extract detail from their.\n",
    "parent_tag = repo_parsed.find_all('h3',{'class':'f3 color-fg-muted text-normal lh-condensed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parent_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the tags which are holding \n",
    "child_tag = parent_tag[0].find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = child_tag[0].text.strip()\n",
    "repo_name = child_tag[1].text.strip()\n",
    "repo_link = child_tag[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_getter = repo_parsed.find_all('span',{'class':'Counter js-social-count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(star_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'83.4k'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_getter[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def star_counter(star_str):\n",
    "    star_str = star_str.strip()\n",
    "    if(star_str[-1]=='k'):\n",
    "        return int(float(star_str[:-1])*1000)\n",
    "    return int(str_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83400"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "star_counter(star_getter[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselink = 'https://github.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defininf a sunction which will get only parsed page and star_getter \n",
    "#and return all the other things like username,repo_name,\n",
    "#repo link and star count of that repo for a single topic page\n",
    "def get_info(parent_tag,star_getter):\n",
    "    child_tag = parent_tag.find_all('a')\n",
    "    username = child_tag[0].text.strip()\n",
    "    repo_name = child_tag[1].text.strip()\n",
    "    repo_link = baselink + child_tag[1]['href']\n",
    "    stars = star_counter(star_getter.text.strip())\n",
    "    return username,repo_name,repo_link,stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_info(parent_tag[0],star_getter[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_page_dict = {\n",
    "    'Username' : [],\n",
    "    'Repo_name' : [],\n",
    "    'Repo_link' : [],\n",
    "    'Stars' : []\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(parent_tag)):\n",
    "    repo_info = get_info(parent_tag[i],star_getter[i])\n",
    "    repo_page_dict['Username'].append(repo_info[0])\n",
    "    repo_page_dict['Repo_name'].append(repo_info[1])\n",
    "    repo_page_dict['Repo_link'].append(repo_info[2])\n",
    "    repo_page_dict['Stars'].append(repo_info[3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselink = 'https://github.com'\n",
    "def get_topic_pages(topic_url):\n",
    "    #loading page\n",
    "    response = requests.get(topic_url)\n",
    "    repo_page = response.text\n",
    "    #checking for error\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "        \n",
    "    #getting parsed page    \n",
    "    repo_parsed = BeautifulSoup(repo_page,'html.parser')\n",
    "    return repo_parsed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#defining a function  for converting star count to integer type from string\n",
    "def star_counter(star_str):\n",
    "    star_str = star_str.strip()\n",
    "    if(star_str[-1]=='k'):\n",
    "        return int(float(star_str[:-1])*1000)\n",
    "    return int(star_str)\n",
    "\n",
    "#defininf a sunction which will get only parsed page and star_getter \n",
    "#and return all the other things like username,repo_name,\n",
    "#repo link and star count of that repo for a single topic page\n",
    "def get_info(parent_tag,star_getter):\n",
    "    child_tag = parent_tag.find_all('a')\n",
    "    username = child_tag[0].text.strip()\n",
    "    repo_name = child_tag[1].text.strip()\n",
    "    repo_link = baselink + child_tag[1]['href']\n",
    "    stars = star_counter(star_getter.text.strip())\n",
    "    return username,repo_name,repo_link,stars\n",
    "\n",
    "def get_topic_repos(repo_parsed):\n",
    "#getting parents tag as username and other details are not under any class\n",
    "    parent_tag = repo_parsed.find_all('h3',\n",
    "            {'class':'f3 color-fg-muted text-normal lh-condensed'})\n",
    "#getting star count for each repo\n",
    "    star_getter = repo_parsed.find_all('span',\n",
    "            {'class':'Counter js-social-count'})\n",
    "\n",
    "\n",
    "#defining a function to create the dictionary of all the details \n",
    "#and convert all the details to data frame\n",
    "    repo_page_dict = {\n",
    "    'Username' : [],\n",
    "    'Repo_name' : [],\n",
    "    'Repo_link' : [],\n",
    "    'Stars' : []\n",
    "    }\n",
    "\n",
    "    for i in range(len(parent_tag)):\n",
    "        repo_info = get_info(parent_tag[i],star_getter[i])\n",
    "        repo_page_dict['Username'].append(repo_info[0])\n",
    "        repo_page_dict['Repo_name'].append(repo_info[1])\n",
    "        repo_page_dict['Repo_link'].append(repo_info[2])\n",
    "        repo_page_dict['Stars'].append(repo_info[3])\n",
    "        \n",
    "        \n",
    "    return pd.DataFrame(repo_page_dict)\n",
    "\n",
    "def scrap_topic(topic_url,path):\n",
    "    if os.path.exists(path):\n",
    "        print('The file {} already exists. Skipping ...'.format(path))\n",
    "        return\n",
    "    topic_repo = get_topic_repos(get_topic_pages(topic_url))\n",
    "    topic_repo.to_csv(path,index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://github.com/topics/angular'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_links[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_repo = get_topic_repos(get_topic_pages(topic_links[5]))\n",
    "# topic_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## writing a single function to:\n",
    "    - get the list of topics from the topic page\n",
    "    - get the list of all top repos from the individual topic pages\n",
    "    -for each topic, create a CSV of the top repos for the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def topic_scrapper():\n",
    "    url = 'https://github.com/topics'\n",
    "    response = requests.get(url)\n",
    "    #by using (.text) we get the page in text formate \n",
    "    page_content = response.text\n",
    "    \n",
    "    parsed_doc = BeautifulSoup(page_content,\"html.parser\")\n",
    "    # now parsed_doc contains the whole page content in html parsed formate\n",
    "\n",
    "    topic_name = parsed_doc.find_all('p',{'class': 'f3 lh-condensed mb-0 mt-1 Link--primary'})\n",
    "    \n",
    "    topic_description = parsed_doc.find_all('p',{'class': 'f5 color-fg-muted mb-0 mt-1'})\n",
    "\n",
    "    topic_link = parsed_doc.find_all('a',{'class':'no-underline flex-1 d-flex flex-column'})\n",
    "    \n",
    "    \n",
    "    # getting all the details in different lists\n",
    "    #topic name list\n",
    "    topic_names=[]\n",
    "    for n in topic_name:\n",
    "        topic_names.append(n.text)\n",
    "    #topic description list    \n",
    "    topic_descp=[]\n",
    "    for d in topic_description:\n",
    "        topic_descp.append(d.text.strip())\n",
    "    #topic link list\n",
    "    topic_links = []\n",
    "    for l in topic_link:\n",
    "        topic_links.append(\"https://github.com\" + l[\"href\"])\n",
    "        \n",
    "    topics_dictionary = {\n",
    "                    \"topic_name\": topic_names,\n",
    "                    \"topic_description\": topic_descp,\n",
    "                    \"topic_link\": topic_links\n",
    "                        }\n",
    "    return pd.DataFrame(topics_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_topic_repos():\n",
    "    print('Scrapping list of topics')\n",
    "    topics_df = topic_scrapper()\n",
    "    \n",
    "    os.makedirs('CSV',exist_ok = True)\n",
    "    \n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scrapping top repositories for \"{}\"'.format(row['topic_name']))\n",
    "        scrap_topic(row['topic_link'],'CSV/{}.csv'.format(row['topic_name']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping list of topics\n",
      "Scrapping top repositories for \"3D\"\n",
      "The file CSV/3D.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Ajax\"\n",
      "The file CSV/Ajax.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Algorithm\"\n",
      "The file CSV/Algorithm.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Amp\"\n",
      "The file CSV/Amp.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Android\"\n",
      "The file CSV/Android.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Angular\"\n",
      "The file CSV/Angular.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Ansible\"\n",
      "The file CSV/Ansible.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"API\"\n",
      "The file CSV/API.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Arduino\"\n",
      "The file CSV/Arduino.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"ASP.NET\"\n",
      "The file CSV/ASP.NET.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Atom\"\n",
      "The file CSV/Atom.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Awesome Lists\"\n",
      "The file CSV/Awesome Lists.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Amazon Web Services\"\n",
      "The file CSV/Amazon Web Services.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Azure\"\n",
      "The file CSV/Azure.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Babel\"\n",
      "The file CSV/Babel.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Bash\"\n",
      "The file CSV/Bash.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Bitcoin\"\n",
      "The file CSV/Bitcoin.csv already exists. Skipping ...\n",
      "Scrapping top repositories for \"Bootstrap\"\n",
      "Scrapping top repositories for \"Bot\"\n",
      "Scrapping top repositories for \"C\"\n",
      "Scrapping top repositories for \"Chrome\"\n",
      "Scrapping top repositories for \"Chrome extension\"\n",
      "Scrapping top repositories for \"Command line interface\"\n",
      "Scrapping top repositories for \"Clojure\"\n",
      "Scrapping top repositories for \"Code quality\"\n",
      "Scrapping top repositories for \"Code review\"\n",
      "Scrapping top repositories for \"Compiler\"\n",
      "Scrapping top repositories for \"Continuous integration\"\n",
      "Scrapping top repositories for \"COVID-19\"\n",
      "Scrapping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrap_topic_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSV file(s) with the extracted information\n",
    "\n",
    "    Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "    Execute the function with different inputs to create a dataset of CSV files.\n",
    "    Verify the information in the CSV files by reading them back using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary to create a dataframe using pandas dataframe\n",
    "topics_dictionary= {\"topic_name\": topic_names,\n",
    "                    \"topic_description\": topic_descp,\n",
    "                   \"topic_link\": topic_links}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe for the topic page\n",
    "topics = pd.DataFrame(topics_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_name</th>\n",
       "      <th>topic_description</th>\n",
       "      <th>topic_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3D</td>\n",
       "      <td>3D modeling is the process of virtually develo...</td>\n",
       "      <td>https://github.com/topics/3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ajax</td>\n",
       "      <td>Ajax is a technique for creating interactive w...</td>\n",
       "      <td>https://github.com/topics/ajax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithm</td>\n",
       "      <td>Algorithms are self-contained sequences that c...</td>\n",
       "      <td>https://github.com/topics/algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amp</td>\n",
       "      <td>Amp is a non-blocking concurrency library for ...</td>\n",
       "      <td>https://github.com/topics/amphp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Android</td>\n",
       "      <td>Android is an operating system built by Google...</td>\n",
       "      <td>https://github.com/topics/android</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  topic_name                                  topic_description  \\\n",
       "0         3D  3D modeling is the process of virtually develo...   \n",
       "1       Ajax  Ajax is a technique for creating interactive w...   \n",
       "2  Algorithm  Algorithms are self-contained sequences that c...   \n",
       "3        Amp  Amp is a non-blocking concurrency library for ...   \n",
       "4    Android  Android is an operating system built by Google...   \n",
       "\n",
       "                            topic_link  \n",
       "0         https://github.com/topics/3d  \n",
       "1       https://github.com/topics/ajax  \n",
       "2  https://github.com/topics/algorithm  \n",
       "3      https://github.com/topics/amphp  \n",
       "4    https://github.com/topics/android  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Username</th>\n",
       "      <th>Repo_name</th>\n",
       "      <th>Repo_link</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mrdoob</td>\n",
       "      <td>three.js</td>\n",
       "      <td>https://github.com/mrdoob/three.js</td>\n",
       "      <td>83400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>libgdx</td>\n",
       "      <td>libgdx</td>\n",
       "      <td>https://github.com/libgdx/libgdx</td>\n",
       "      <td>20200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pmndrs</td>\n",
       "      <td>react-three-fiber</td>\n",
       "      <td>https://github.com/pmndrs/react-three-fiber</td>\n",
       "      <td>18600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BabylonJS</td>\n",
       "      <td>Babylon.js</td>\n",
       "      <td>https://github.com/BabylonJS/Babylon.js</td>\n",
       "      <td>17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aframevr</td>\n",
       "      <td>aframe</td>\n",
       "      <td>https://github.com/aframevr/aframe</td>\n",
       "      <td>14300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Username          Repo_name                                    Repo_link  \\\n",
       "0     mrdoob           three.js           https://github.com/mrdoob/three.js   \n",
       "1     libgdx             libgdx             https://github.com/libgdx/libgdx   \n",
       "2     pmndrs  react-three-fiber  https://github.com/pmndrs/react-three-fiber   \n",
       "3  BabylonJS         Babylon.js      https://github.com/BabylonJS/Babylon.js   \n",
       "4   aframevr             aframe           https://github.com/aframevr/aframe   \n",
       "\n",
       "   Stars  \n",
       "0  83400  \n",
       "1  20200  \n",
       "2  18600  \n",
       "3  17700  \n",
       "4  14300  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe for the 1st repo page\n",
    "repos = pd.DataFrame(repo_page_dict)\n",
    "repos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document and share your work\n",
    "\n",
    "    Add proper headings and documentation in your Jupyter notebook.\n",
    "    Publish your Jupyter notebook to your Jovian profile\n",
    "    (Optional) Write a blog post about your project and share it online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
